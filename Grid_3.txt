exploring: random action
total reward:-2
Episode: 0.01Total Reward: -2Total Steps: 1Epsilon: 1.0
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
exploring: random action
total reward:9
exploring: random action
total reward:7
Episode: 1.01Total Reward: 7Total Steps: 2Epsilon: 1.0
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
exploring: random action
total reward:-1
exploring: random action
total reward:8
exploring: random action
total reward:7
exploring: random action
total reward:16
exploring: random action
total reward:25
exploring: random action
total reward:23
Episode: 2.01Total Reward: 23Total Steps: 6Epsilon: 1.0
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
exploring: random action
total reward:-2
Episode: 3.01Total Reward: -2Total Steps: 1Epsilon: 1.0
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
exploring: random action
total reward:9
exploring: random action
total reward:18
exploring: random action
total reward:16
Episode: 4.01Total Reward: 16Total Steps: 3Epsilon: 1.0
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
exploring: random action
total reward:-1
exploring: random action
total reward:8
exploring: random action
total reward:17
exploring: random action
total reward:26
exploring: random action
total reward:24
Episode: 5.01Total Reward: 24Total Steps: 5Epsilon: 1.0
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:-2
Episode: 6.01Total Reward: -2Total Steps: 1Epsilon: 0.9
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:7
Episode: 7.01Total Reward: 7Total Steps: 2Epsilon: 0.81
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:-2
Episode: 8.01Total Reward: -2Total Steps: 1Epsilon: 0.7290000000000001
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:-1
exploiting: q-values predicted from network
-------------------------------
total reward:8
exploiting: q-values predicted from network
-------------------------------
total reward:17
exploring: random action
total reward:16
exploring: random action
total reward:25
exploring: random action
total reward:24
exploring: random action
total reward:33
exploring: random action
total reward:32
exploring: random action
total reward:41
exploiting: q-values predicted from network
-------------------------------
total reward:50
exploiting: q-values predicted from network
-------------------------------
total reward:59
exploring: random action
total reward:58
exploring: random action
total reward:57
exploiting: q-values predicted from network
-------------------------------
total reward:66
exploring: random action
total reward:75
exploring: random action
total reward:74
exploring: random action
total reward:83
exploring: random action
total reward:81
Episode: 9.01Total Reward: 81Total Steps: 18Epsilon: 0.6561000000000001
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:8
exploiting: q-values predicted from network
-------------------------------
total reward:17
exploring: random action
total reward:26
exploiting: q-values predicted from network
-------------------------------
total reward:24
Episode: 10.01Total Reward: 24Total Steps: 5Epsilon: 0.5904900000000002
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:9
exploring: random action
total reward:7
Episode: 11.01Total Reward: 7Total Steps: 2Epsilon: 0.5314410000000002
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:18
exploring: random action
total reward:27
exploring: random action
total reward:26
exploring: random action
total reward:35
exploiting: q-values predicted from network
-------------------------------
total reward:33
Episode: 12.01Total Reward: 33Total Steps: 6Epsilon: 0.47829690000000014
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:16
Episode: 13.01Total Reward: 16Total Steps: 3Epsilon: 0.43046721000000016
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploring: random action
total reward:17
exploiting: q-values predicted from network
-------------------------------
total reward:26
exploring: random action
total reward:35
exploiting: q-values predicted from network
-------------------------------
total reward:44
exploiting: q-values predicted from network
-------------------------------
total reward:53
exploiting: q-values predicted from network
-------------------------------
total reward:51
Episode: 14.01Total Reward: 51Total Steps: 8Epsilon: 0.38742048900000015
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:-1
exploring: random action
total reward:-3
Episode: 15.01Total Reward: -3Total Steps: 2Epsilon: 0.34867844010000015
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:43
Episode: 16.01Total Reward: 43Total Steps: 6Epsilon: 0.31381059609000017
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:34
Episode: 17.01Total Reward: 34Total Steps: 5Epsilon: 0.28242953648100017
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:7
Episode: 18.01Total Reward: 7Total Steps: 2Epsilon: 0.25418658283290013
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:16
Episode: 19.01Total Reward: 16Total Steps: 3Epsilon: 0.22876792454961012
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploring: random action
total reward:25
Episode: 20.01Total Reward: 25Total Steps: 4Epsilon: 0.2058911320946491
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:25
Episode: 21.01Total Reward: 25Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploring: random action
total reward:25
Episode: 22.01Total Reward: 25Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:-1
exploiting: q-values predicted from network
-------------------------------
total reward:-2
exploiting: q-values predicted from network
-------------------------------
total reward:7
exploiting: q-values predicted from network
-------------------------------
total reward:16
exploiting: q-values predicted from network
-------------------------------
total reward:25
exploring: random action
total reward:23
Episode: 23.01Total Reward: 23Total Steps: 6Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:-1
exploiting: q-values predicted from network
-------------------------------
total reward:-2
exploiting: q-values predicted from network
-------------------------------
total reward:-3
exploring: random action
total reward:-5
Episode: 24.01Total Reward: -5Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:17
exploiting: q-values predicted from network
-------------------------------
total reward:16
exploiting: q-values predicted from network
-------------------------------
total reward:15
exploiting: q-values predicted from network
-------------------------------
total reward:14
exploring: random action
total reward:12
Episode: 25.01Total Reward: 12Total Steps: 7Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:-1
exploring: random action
total reward:-3
Episode: 26.01Total Reward: -3Total Steps: 2Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploring: random action
total reward:79
Episode: 27.01Total Reward: 79Total Steps: 10Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploring: random action
total reward:16
Episode: 28.01Total Reward: 16Total Steps: 3Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploring: random action
total reward:27
exploring: random action
total reward:25
Episode: 29.01Total Reward: 25Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:-1
exploiting: q-values predicted from network
-------------------------------
total reward:8
exploring: random action
total reward:6
Episode: 30.01Total Reward: 6Total Steps: 3Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploring: random action
total reward:26
exploring: random action
total reward:35
exploiting: q-values predicted from network
-------------------------------
total reward:44
exploring: random action
total reward:42
Episode: 31.01Total Reward: 42Total Steps: 7Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:-1
exploiting: q-values predicted from network
-------------------------------
total reward:8
exploiting: q-values predicted from network
-------------------------------
total reward:17
exploring: random action
total reward:16
exploiting: q-values predicted from network
-------------------------------
total reward:25
exploiting: q-values predicted from network
-------------------------------
total reward:34
exploiting: q-values predicted from network
-------------------------------
total reward:43
exploiting: q-values predicted from network
-------------------------------
total reward:52
exploiting: q-values predicted from network
-------------------------------
total reward:61
exploiting: q-values predicted from network
-------------------------------
total reward:59
Episode: 32.01Total Reward: 59Total Steps: 10Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:25
Episode: 33.01Total Reward: 25Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:25
Episode: 34.01Total Reward: 25Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:25
Episode: 35.01Total Reward: 25Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:25
Episode: 36.01Total Reward: 25Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:25
Episode: 37.01Total Reward: 25Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:8
exploiting: q-values predicted from network
-------------------------------
total reward:17
exploiting: q-values predicted from network
-------------------------------
total reward:15
Episode: 38.01Total Reward: 15Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:7
Episode: 39.01Total Reward: 7Total Steps: 2Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:16
Episode: 40.01Total Reward: 16Total Steps: 3Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:16
Episode: 41.01Total Reward: 16Total Steps: 3Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:16
Episode: 42.01Total Reward: 16Total Steps: 3Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploring: random action
total reward:17
exploring: random action
total reward:16
exploiting: q-values predicted from network
-------------------------------
total reward:25
exploiting: q-values predicted from network
-------------------------------
total reward:23
Episode: 43.01Total Reward: 23Total Steps: 6Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:-1
exploiting: q-values predicted from network
-------------------------------
total reward:-3
Episode: 44.01Total Reward: -3Total Steps: 2Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:-1
exploiting: q-values predicted from network
-------------------------------
total reward:-3
Episode: 45.01Total Reward: -3Total Steps: 2Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:-1
exploiting: q-values predicted from network
-------------------------------
total reward:-3
Episode: 46.01Total Reward: -3Total Steps: 2Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploring: random action
total reward:17
exploiting: q-values predicted from network
-------------------------------
total reward:15
Episode: 47.01Total Reward: 15Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploring: random action
total reward:27
exploring: random action
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploring: random action
total reward:62
exploiting: q-values predicted from network
-------------------------------
total reward:71
exploiting: q-values predicted from network
-------------------------------
total reward:69
Episode: 48.01Total Reward: 69Total Steps: 10Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:25
Episode: 49.01Total Reward: 25Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploring: random action
total reward:36
exploring: random action
total reward:45
exploring: random action
total reward:44
exploiting: q-values predicted from network
-------------------------------
total reward:53
exploiting: q-values predicted from network
-------------------------------
total reward:62
exploiting: q-values predicted from network
-------------------------------
total reward:71
exploiting: q-values predicted from network
-------------------------------
total reward:80
exploiting: q-values predicted from network
-------------------------------
total reward:89
exploiting: q-values predicted from network
-------------------------------
total reward:98
exploiting: q-values predicted from network
-------------------------------
total reward:107
exploiting: q-values predicted from network
-------------------------------
total reward:116
exploiting: q-values predicted from network
-------------------------------
total reward:125
exploiting: q-values predicted from network
-------------------------------
total reward:134
exploiting: q-values predicted from network
-------------------------------
total reward:143
exploiting: q-values predicted from network
-------------------------------
total reward:152
exploiting: q-values predicted from network
-------------------------------
total reward:161
exploiting: q-values predicted from network
-------------------------------
total reward:170
exploiting: q-values predicted from network
-------------------------------
total reward:179
exploiting: q-values predicted from network
-------------------------------
total reward:188
exploiting: q-values predicted from network
-------------------------------
total reward:197
exploiting: q-values predicted from network
-------------------------------
total reward:206
exploring: random action
total reward:215
exploring: random action
total reward:214
exploiting: q-values predicted from network
-------------------------------
total reward:223
exploiting: q-values predicted from network
-------------------------------
total reward:232
exploiting: q-values predicted from network
-------------------------------
total reward:241
exploring: random action
total reward:239
Episode: 50.01Total Reward: 239Total Steps: 30Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:9
exploring: random action
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:16
Episode: 51.01Total Reward: 16Total Steps: 3Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:8
exploiting: q-values predicted from network
-------------------------------
total reward:6
Episode: 52.01Total Reward: 6Total Steps: 3Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:7
Episode: 53.01Total Reward: 7Total Steps: 2Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:7
Episode: 54.01Total Reward: 7Total Steps: 2Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploring: random action
total reward:26
exploiting: q-values predicted from network
-------------------------------
total reward:35
exploiting: q-values predicted from network
-------------------------------
total reward:44
exploiting: q-values predicted from network
-------------------------------
total reward:53
exploiting: q-values predicted from network
-------------------------------
total reward:62
exploiting: q-values predicted from network
-------------------------------
total reward:71
exploiting: q-values predicted from network
-------------------------------
total reward:80
exploiting: q-values predicted from network
-------------------------------
total reward:89
exploiting: q-values predicted from network
-------------------------------
total reward:98
exploiting: q-values predicted from network
-------------------------------
total reward:107
exploiting: q-values predicted from network
-------------------------------
total reward:116
exploiting: q-values predicted from network
-------------------------------
total reward:125
exploiting: q-values predicted from network
-------------------------------
total reward:134
exploiting: q-values predicted from network
-------------------------------
total reward:143
exploring: random action
total reward:141
Episode: 55.01Total Reward: 141Total Steps: 18Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploring: random action
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploiting: q-values predicted from network
-------------------------------
total reward:90
exploiting: q-values predicted from network
-------------------------------
total reward:99
exploiting: q-values predicted from network
-------------------------------
total reward:108
exploiting: q-values predicted from network
-------------------------------
total reward:117
exploiting: q-values predicted from network
-------------------------------
total reward:126
exploiting: q-values predicted from network
-------------------------------
total reward:135
exploiting: q-values predicted from network
-------------------------------
total reward:144
exploring: random action
total reward:153
exploiting: q-values predicted from network
-------------------------------
total reward:162
exploiting: q-values predicted from network
-------------------------------
total reward:171
exploiting: q-values predicted from network
-------------------------------
total reward:180
exploiting: q-values predicted from network
-------------------------------
total reward:189
exploiting: q-values predicted from network
-------------------------------
total reward:198
exploring: random action
total reward:197
exploiting: q-values predicted from network
-------------------------------
total reward:206
exploiting: q-values predicted from network
-------------------------------
total reward:215
exploring: random action
total reward:214
exploring: random action
total reward:213
exploiting: q-values predicted from network
-------------------------------
total reward:222
exploring: random action
total reward:220
Episode: 56.01Total Reward: 220Total Steps: 29Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploring: random action
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploring: random action
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploiting: q-values predicted from network
-------------------------------
total reward:90
exploiting: q-values predicted from network
-------------------------------
total reward:99
exploiting: q-values predicted from network
-------------------------------
total reward:108
exploiting: q-values predicted from network
-------------------------------
total reward:117
exploiting: q-values predicted from network
-------------------------------
total reward:126
exploiting: q-values predicted from network
-------------------------------
total reward:135
exploiting: q-values predicted from network
-------------------------------
total reward:144
exploring: random action
total reward:143
exploiting: q-values predicted from network
-------------------------------
total reward:152
exploiting: q-values predicted from network
-------------------------------
total reward:161
exploiting: q-values predicted from network
-------------------------------
total reward:170
exploiting: q-values predicted from network
-------------------------------
total reward:179
exploiting: q-values predicted from network
-------------------------------
total reward:188
exploiting: q-values predicted from network
-------------------------------
total reward:197
exploiting: q-values predicted from network
-------------------------------
total reward:206
exploiting: q-values predicted from network
-------------------------------
total reward:215
exploring: random action
total reward:224
exploiting: q-values predicted from network
-------------------------------
total reward:233
exploiting: q-values predicted from network
-------------------------------
total reward:242
exploiting: q-values predicted from network
-------------------------------
total reward:251
exploiting: q-values predicted from network
-------------------------------
total reward:260
exploiting: q-values predicted from network
-------------------------------
total reward:269
exploiting: q-values predicted from network
-------------------------------
total reward:278
exploring: random action
total reward:287
exploiting: q-values predicted from network
-------------------------------
total reward:296
exploiting: q-values predicted from network
-------------------------------
total reward:305
exploiting: q-values predicted from network
-------------------------------
total reward:314
exploiting: q-values predicted from network
-------------------------------
total reward:323
exploiting: q-values predicted from network
-------------------------------
total reward:332
exploiting: q-values predicted from network
-------------------------------
total reward:341
exploring: random action
total reward:350
exploiting: q-values predicted from network
-------------------------------
total reward:359
exploiting: q-values predicted from network
-------------------------------
total reward:368
exploiting: q-values predicted from network
-------------------------------
total reward:377
exploiting: q-values predicted from network
-------------------------------
total reward:386
exploiting: q-values predicted from network
-------------------------------
total reward:395
exploiting: q-values predicted from network
-------------------------------
total reward:404
exploiting: q-values predicted from network
-------------------------------
total reward:413
exploring: random action
total reward:422
exploiting: q-values predicted from network
-------------------------------
total reward:431
exploiting: q-values predicted from network
-------------------------------
total reward:440
exploiting: q-values predicted from network
-------------------------------
total reward:449
exploiting: q-values predicted from network
-------------------------------
total reward:458
exploiting: q-values predicted from network
-------------------------------
total reward:467
exploiting: q-values predicted from network
-------------------------------
total reward:476
exploiting: q-values predicted from network
-------------------------------
total reward:485
exploiting: q-values predicted from network
-------------------------------
total reward:494
exploiting: q-values predicted from network
-------------------------------
total reward:503
exploiting: q-values predicted from network
-------------------------------
total reward:512
exploiting: q-values predicted from network
-------------------------------
total reward:521
exploring: random action
total reward:520
exploiting: q-values predicted from network
-------------------------------
total reward:529
exploiting: q-values predicted from network
-------------------------------
total reward:538
exploiting: q-values predicted from network
-------------------------------
total reward:547
exploiting: q-values predicted from network
-------------------------------
total reward:556
exploiting: q-values predicted from network
-------------------------------
total reward:565
exploiting: q-values predicted from network
-------------------------------
total reward:574
exploring: random action
total reward:573
exploiting: q-values predicted from network
-------------------------------
total reward:582
exploiting: q-values predicted from network
-------------------------------
total reward:591
exploiting: q-values predicted from network
-------------------------------
total reward:600
exploiting: q-values predicted from network
-------------------------------
total reward:609
exploiting: q-values predicted from network
-------------------------------
total reward:618
exploring: random action
total reward:627
exploiting: q-values predicted from network
-------------------------------
total reward:636
exploiting: q-values predicted from network
-------------------------------
total reward:645
exploiting: q-values predicted from network
-------------------------------
total reward:654
exploiting: q-values predicted from network
-------------------------------
total reward:663
exploring: random action
total reward:672
exploiting: q-values predicted from network
-------------------------------
total reward:681
exploiting: q-values predicted from network
-------------------------------
total reward:690
exploring: random action
total reward:699
exploiting: q-values predicted from network
-------------------------------
total reward:708
exploiting: q-values predicted from network
-------------------------------
total reward:717
exploiting: q-values predicted from network
-------------------------------
total reward:726
exploiting: q-values predicted from network
-------------------------------
total reward:735
exploiting: q-values predicted from network
-------------------------------
total reward:744
exploiting: q-values predicted from network
-------------------------------
total reward:753
exploiting: q-values predicted from network
-------------------------------
total reward:762
exploring: random action
total reward:761
exploring: random action
total reward:770
exploring: random action
total reward:769
exploiting: q-values predicted from network
-------------------------------
total reward:778
exploiting: q-values predicted from network
-------------------------------
total reward:787
exploiting: q-values predicted from network
-------------------------------
total reward:796
exploiting: q-values predicted from network
-------------------------------
total reward:805
exploiting: q-values predicted from network
-------------------------------
total reward:814
exploiting: q-values predicted from network
-------------------------------
total reward:823
exploiting: q-values predicted from network
-------------------------------
total reward:832
exploiting: q-values predicted from network
-------------------------------
total reward:841
exploring: random action
total reward:839
Episode: 57.01Total Reward: 839Total Steps: 100Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:7
Episode: 58.01Total Reward: 7Total Steps: 2Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:-1
exploiting: q-values predicted from network
-------------------------------
total reward:8
exploiting: q-values predicted from network
-------------------------------
total reward:17
exploiting: q-values predicted from network
-------------------------------
total reward:26
exploiting: q-values predicted from network
-------------------------------
total reward:35
exploring: random action
total reward:44
exploiting: q-values predicted from network
-------------------------------
total reward:53
exploiting: q-values predicted from network
-------------------------------
total reward:62
exploiting: q-values predicted from network
-------------------------------
total reward:71
exploiting: q-values predicted from network
-------------------------------
total reward:80
exploiting: q-values predicted from network
-------------------------------
total reward:89
exploring: random action
total reward:87
Episode: 59.01Total Reward: 87Total Steps: 12Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploring: random action
total reward:43
Episode: 60.01Total Reward: 43Total Steps: 6Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploring: random action
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploiting: q-values predicted from network
-------------------------------
total reward:90
exploring: random action
total reward:88
Episode: 61.01Total Reward: 88Total Steps: 11Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploring: random action
total reward:52
Episode: 62.01Total Reward: 52Total Steps: 7Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:43
Episode: 63.01Total Reward: 43Total Steps: 6Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:8
exploring: random action
total reward:17
exploiting: q-values predicted from network
-------------------------------
total reward:26
exploiting: q-values predicted from network
-------------------------------
total reward:35
exploiting: q-values predicted from network
-------------------------------
total reward:33
Episode: 64.01Total Reward: 33Total Steps: 6Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploring: random action
total reward:44
exploiting: q-values predicted from network
-------------------------------
total reward:53
exploiting: q-values predicted from network
-------------------------------
total reward:62
exploiting: q-values predicted from network
-------------------------------
total reward:71
exploring: random action
total reward:80
exploring: random action
total reward:89
exploiting: q-values predicted from network
-------------------------------
total reward:98
exploiting: q-values predicted from network
-------------------------------
total reward:107
exploiting: q-values predicted from network
-------------------------------
total reward:116
exploiting: q-values predicted from network
-------------------------------
total reward:125
exploiting: q-values predicted from network
-------------------------------
total reward:134
exploiting: q-values predicted from network
-------------------------------
total reward:143
exploiting: q-values predicted from network
-------------------------------
total reward:152
exploiting: q-values predicted from network
-------------------------------
total reward:161
exploiting: q-values predicted from network
-------------------------------
total reward:170
exploiting: q-values predicted from network
-------------------------------
total reward:179
exploiting: q-values predicted from network
-------------------------------
total reward:188
exploiting: q-values predicted from network
-------------------------------
total reward:197
exploring: random action
total reward:206
exploiting: q-values predicted from network
-------------------------------
total reward:215
exploiting: q-values predicted from network
-------------------------------
total reward:224
exploring: random action
total reward:223
exploiting: q-values predicted from network
-------------------------------
total reward:232
exploiting: q-values predicted from network
-------------------------------
total reward:241
exploiting: q-values predicted from network
-------------------------------
total reward:250
exploiting: q-values predicted from network
-------------------------------
total reward:259
exploiting: q-values predicted from network
-------------------------------
total reward:268
exploiting: q-values predicted from network
-------------------------------
total reward:277
exploiting: q-values predicted from network
-------------------------------
total reward:286
exploring: random action
total reward:285
exploiting: q-values predicted from network
-------------------------------
total reward:294
exploiting: q-values predicted from network
-------------------------------
total reward:303
exploiting: q-values predicted from network
-------------------------------
total reward:312
exploiting: q-values predicted from network
-------------------------------
total reward:321
exploring: random action
total reward:319
Episode: 65.01Total Reward: 319Total Steps: 40Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploring: random action
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploiting: q-values predicted from network
-------------------------------
total reward:90
exploiting: q-values predicted from network
-------------------------------
total reward:99
exploiting: q-values predicted from network
-------------------------------
total reward:108
exploiting: q-values predicted from network
-------------------------------
total reward:117
exploiting: q-values predicted from network
-------------------------------
total reward:126
exploiting: q-values predicted from network
-------------------------------
total reward:135
exploiting: q-values predicted from network
-------------------------------
total reward:144
exploiting: q-values predicted from network
-------------------------------
total reward:153
exploiting: q-values predicted from network
-------------------------------
total reward:162
exploiting: q-values predicted from network
-------------------------------
total reward:171
exploiting: q-values predicted from network
-------------------------------
total reward:180
exploiting: q-values predicted from network
-------------------------------
total reward:189
exploiting: q-values predicted from network
-------------------------------
total reward:198
exploring: random action
total reward:196
Episode: 66.01Total Reward: 196Total Steps: 23Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploring: random action
total reward:71
exploiting: q-values predicted from network
-------------------------------
total reward:80
exploring: random action
total reward:79
exploiting: q-values predicted from network
-------------------------------
total reward:88
exploring: random action
total reward:86
Episode: 67.01Total Reward: 86Total Steps: 13Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploring: random action
total reward:17
exploiting: q-values predicted from network
-------------------------------
total reward:26
exploring: random action
total reward:35
exploiting: q-values predicted from network
-------------------------------
total reward:44
exploiting: q-values predicted from network
-------------------------------
total reward:53
exploiting: q-values predicted from network
-------------------------------
total reward:62
exploring: random action
total reward:61
exploiting: q-values predicted from network
-------------------------------
total reward:70
exploiting: q-values predicted from network
-------------------------------
total reward:79
exploiting: q-values predicted from network
-------------------------------
total reward:88
exploiting: q-values predicted from network
-------------------------------
total reward:97
exploiting: q-values predicted from network
-------------------------------
total reward:106
exploiting: q-values predicted from network
-------------------------------
total reward:115
exploring: random action
total reward:113
Episode: 68.01Total Reward: 113Total Steps: 16Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploiting: q-values predicted from network
-------------------------------
total reward:90
exploiting: q-values predicted from network
-------------------------------
total reward:99
exploiting: q-values predicted from network
-------------------------------
total reward:108
exploring: random action
total reward:106
Episode: 69.01Total Reward: 106Total Steps: 13Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploiting: q-values predicted from network
-------------------------------
total reward:90
exploiting: q-values predicted from network
-------------------------------
total reward:99
exploiting: q-values predicted from network
-------------------------------
total reward:108
exploiting: q-values predicted from network
-------------------------------
total reward:117
exploring: random action
total reward:116
exploiting: q-values predicted from network
-------------------------------
total reward:125
exploiting: q-values predicted from network
-------------------------------
total reward:134
exploring: random action
total reward:143
exploiting: q-values predicted from network
-------------------------------
total reward:152
exploiting: q-values predicted from network
-------------------------------
total reward:161
exploiting: q-values predicted from network
-------------------------------
total reward:170
exploring: random action
total reward:179
exploiting: q-values predicted from network
-------------------------------
total reward:188
exploiting: q-values predicted from network
-------------------------------
total reward:197
exploiting: q-values predicted from network
-------------------------------
total reward:206
exploiting: q-values predicted from network
-------------------------------
total reward:215
exploiting: q-values predicted from network
-------------------------------
total reward:224
exploiting: q-values predicted from network
-------------------------------
total reward:233
exploiting: q-values predicted from network
-------------------------------
total reward:242
exploiting: q-values predicted from network
-------------------------------
total reward:251
exploiting: q-values predicted from network
-------------------------------
total reward:260
exploiting: q-values predicted from network
-------------------------------
total reward:269
exploiting: q-values predicted from network
-------------------------------
total reward:278
exploiting: q-values predicted from network
-------------------------------
total reward:287
exploiting: q-values predicted from network
-------------------------------
total reward:296
exploiting: q-values predicted from network
-------------------------------
total reward:305
exploiting: q-values predicted from network
-------------------------------
total reward:314
exploring: random action
total reward:312
Episode: 70.01Total Reward: 312Total Steps: 37Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:-2
Episode: 71.01Total Reward: -2Total Steps: 1Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploiting: q-values predicted from network
-------------------------------
total reward:90
exploiting: q-values predicted from network
-------------------------------
total reward:99
exploring: random action
total reward:108
exploiting: q-values predicted from network
-------------------------------
total reward:117
exploiting: q-values predicted from network
-------------------------------
total reward:126
exploiting: q-values predicted from network
-------------------------------
total reward:135
exploring: random action
total reward:133
Episode: 72.01Total Reward: 133Total Steps: 16Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploring: random action
total reward:16
Episode: 73.01Total Reward: 16Total Steps: 3Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploring: random action
total reward:17
exploring: random action
total reward:15
Episode: 74.01Total Reward: 15Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:25
Episode: 75.01Total Reward: 25Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:25
Episode: 76.01Total Reward: 25Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploring: random action
total reward:17
exploiting: q-values predicted from network
-------------------------------
total reward:26
exploiting: q-values predicted from network
-------------------------------
total reward:35
exploiting: q-values predicted from network
-------------------------------
total reward:44
exploiting: q-values predicted from network
-------------------------------
total reward:42
Episode: 77.01Total Reward: 42Total Steps: 7Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:-2
Episode: 78.01Total Reward: -2Total Steps: 1Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:25
Episode: 79.01Total Reward: 25Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:8
exploiting: q-values predicted from network
-------------------------------
total reward:17
exploring: random action
total reward:15
Episode: 80.01Total Reward: 15Total Steps: 4Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:7
Episode: 81.01Total Reward: 7Total Steps: 2Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploring: random action
total reward:26
exploiting: q-values predicted from network
-------------------------------
total reward:35
exploiting: q-values predicted from network
-------------------------------
total reward:33
Episode: 82.01Total Reward: 33Total Steps: 6Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploring: random action
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploring: random action
total reward:70
Episode: 83.01Total Reward: 70Total Steps: 9Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploring: random action
total reward:9
exploring: random action
total reward:7
Episode: 84.01Total Reward: 7Total Steps: 2Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploring: random action
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploring: random action
total reward:80
exploiting: q-values predicted from network
-------------------------------
total reward:89
exploring: random action
total reward:88
exploiting: q-values predicted from network
-------------------------------
total reward:97
exploiting: q-values predicted from network
-------------------------------
total reward:106
exploiting: q-values predicted from network
-------------------------------
total reward:115
exploiting: q-values predicted from network
-------------------------------
total reward:124
exploring: random action
total reward:122
Episode: 85.01Total Reward: 122Total Steps: 17Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploring: random action
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploring: random action
total reward:44
exploiting: q-values predicted from network
-------------------------------
total reward:53
exploring: random action
total reward:62
exploiting: q-values predicted from network
-------------------------------
total reward:71
exploiting: q-values predicted from network
-------------------------------
total reward:80
exploiting: q-values predicted from network
-------------------------------
total reward:89
exploiting: q-values predicted from network
-------------------------------
total reward:98
exploiting: q-values predicted from network
-------------------------------
total reward:107
exploiting: q-values predicted from network
-------------------------------
total reward:116
exploiting: q-values predicted from network
-------------------------------
total reward:125
exploiting: q-values predicted from network
-------------------------------
total reward:134
exploiting: q-values predicted from network
-------------------------------
total reward:143
exploiting: q-values predicted from network
-------------------------------
total reward:152
exploiting: q-values predicted from network
-------------------------------
total reward:161
exploiting: q-values predicted from network
-------------------------------
total reward:170
exploiting: q-values predicted from network
-------------------------------
total reward:179
exploiting: q-values predicted from network
-------------------------------
total reward:188
exploiting: q-values predicted from network
-------------------------------
total reward:197
exploiting: q-values predicted from network
-------------------------------
total reward:206
exploiting: q-values predicted from network
-------------------------------
total reward:215
exploiting: q-values predicted from network
-------------------------------
total reward:224
exploiting: q-values predicted from network
-------------------------------
total reward:233
exploiting: q-values predicted from network
-------------------------------
total reward:242
exploring: random action
total reward:241
exploiting: q-values predicted from network
-------------------------------
total reward:250
exploring: random action
total reward:259
exploiting: q-values predicted from network
-------------------------------
total reward:268
exploiting: q-values predicted from network
-------------------------------
total reward:277
exploring: random action
total reward:275
Episode: 86.01Total Reward: 275Total Steps: 34Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploring: random action
total reward:26
exploiting: q-values predicted from network
-------------------------------
total reward:35
exploiting: q-values predicted from network
-------------------------------
total reward:44
exploring: random action
total reward:42
Episode: 87.01Total Reward: 42Total Steps: 7Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploring: random action
total reward:16
Episode: 88.01Total Reward: 16Total Steps: 3Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploiting: q-values predicted from network
-------------------------------
total reward:90
exploiting: q-values predicted from network
-------------------------------
total reward:99
exploiting: q-values predicted from network
-------------------------------
total reward:108
exploiting: q-values predicted from network
-------------------------------
total reward:117
exploiting: q-values predicted from network
-------------------------------
total reward:126
exploiting: q-values predicted from network
-------------------------------
total reward:135
exploiting: q-values predicted from network
-------------------------------
total reward:144
exploiting: q-values predicted from network
-------------------------------
total reward:153
exploiting: q-values predicted from network
-------------------------------
total reward:162
exploiting: q-values predicted from network
-------------------------------
total reward:171
exploring: random action
total reward:180
exploiting: q-values predicted from network
-------------------------------
total reward:189
exploiting: q-values predicted from network
-------------------------------
total reward:198
exploiting: q-values predicted from network
-------------------------------
total reward:207
exploiting: q-values predicted from network
-------------------------------
total reward:216
exploiting: q-values predicted from network
-------------------------------
total reward:225
exploring: random action
total reward:223
Episode: 89.01Total Reward: 223Total Steps: 26Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploiting: q-values predicted from network
-------------------------------
total reward:90
exploring: random action
total reward:89
exploiting: q-values predicted from network
-------------------------------
total reward:98
exploiting: q-values predicted from network
-------------------------------
total reward:107
exploiting: q-values predicted from network
-------------------------------
total reward:116
exploiting: q-values predicted from network
-------------------------------
total reward:125
exploiting: q-values predicted from network
-------------------------------
total reward:134
exploiting: q-values predicted from network
-------------------------------
total reward:143
exploiting: q-values predicted from network
-------------------------------
total reward:152
exploiting: q-values predicted from network
-------------------------------
total reward:161
exploiting: q-values predicted from network
-------------------------------
total reward:170
exploiting: q-values predicted from network
-------------------------------
total reward:179
exploiting: q-values predicted from network
-------------------------------
total reward:188
exploiting: q-values predicted from network
-------------------------------
total reward:197
exploiting: q-values predicted from network
-------------------------------
total reward:206
exploiting: q-values predicted from network
-------------------------------
total reward:215
exploiting: q-values predicted from network
-------------------------------
total reward:224
exploiting: q-values predicted from network
-------------------------------
total reward:233
exploiting: q-values predicted from network
-------------------------------
total reward:242
exploiting: q-values predicted from network
-------------------------------
total reward:251
exploiting: q-values predicted from network
-------------------------------
total reward:260
exploiting: q-values predicted from network
-------------------------------
total reward:269
exploiting: q-values predicted from network
-------------------------------
total reward:278
exploring: random action
total reward:287
exploiting: q-values predicted from network
-------------------------------
total reward:296
exploiting: q-values predicted from network
-------------------------------
total reward:305
exploiting: q-values predicted from network
-------------------------------
total reward:314
exploiting: q-values predicted from network
-------------------------------
total reward:323
exploiting: q-values predicted from network
-------------------------------
total reward:332
exploring: random action
total reward:341
exploiting: q-values predicted from network
-------------------------------
total reward:350
exploiting: q-values predicted from network
-------------------------------
total reward:359
exploring: random action
total reward:358
exploiting: q-values predicted from network
-------------------------------
total reward:367
exploiting: q-values predicted from network
-------------------------------
total reward:376
exploiting: q-values predicted from network
-------------------------------
total reward:385
exploiting: q-values predicted from network
-------------------------------
total reward:394
exploiting: q-values predicted from network
-------------------------------
total reward:403
exploiting: q-values predicted from network
-------------------------------
total reward:412
exploiting: q-values predicted from network
-------------------------------
total reward:421
exploiting: q-values predicted from network
-------------------------------
total reward:430
exploring: random action
total reward:428
Episode: 90.01Total Reward: 428Total Steps: 51Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploring: random action
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploiting: q-values predicted from network
-------------------------------
total reward:90
exploiting: q-values predicted from network
-------------------------------
total reward:99
exploiting: q-values predicted from network
-------------------------------
total reward:108
exploiting: q-values predicted from network
-------------------------------
total reward:117
exploring: random action
total reward:116
exploiting: q-values predicted from network
-------------------------------
total reward:125
exploiting: q-values predicted from network
-------------------------------
total reward:134
exploiting: q-values predicted from network
-------------------------------
total reward:143
exploiting: q-values predicted from network
-------------------------------
total reward:152
exploiting: q-values predicted from network
-------------------------------
total reward:161
exploring: random action
total reward:170
exploiting: q-values predicted from network
-------------------------------
total reward:179
exploiting: q-values predicted from network
-------------------------------
total reward:188
exploring: random action
total reward:187
exploiting: q-values predicted from network
-------------------------------
total reward:196
exploring: random action
total reward:195
exploiting: q-values predicted from network
-------------------------------
total reward:204
exploiting: q-values predicted from network
-------------------------------
total reward:213
exploiting: q-values predicted from network
-------------------------------
total reward:222
exploiting: q-values predicted from network
-------------------------------
total reward:231
exploiting: q-values predicted from network
-------------------------------
total reward:240
exploiting: q-values predicted from network
-------------------------------
total reward:249
exploiting: q-values predicted from network
-------------------------------
total reward:258
exploring: random action
total reward:256
Episode: 91.01Total Reward: 256Total Steps: 33Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:7
Episode: 92.01Total Reward: 7Total Steps: 2Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploring: random action
total reward:81
exploiting: q-values predicted from network
-------------------------------
total reward:90
exploiting: q-values predicted from network
-------------------------------
total reward:99
exploiting: q-values predicted from network
-------------------------------
total reward:108
exploiting: q-values predicted from network
-------------------------------
total reward:117
exploiting: q-values predicted from network
-------------------------------
total reward:126
exploring: random action
total reward:125
exploiting: q-values predicted from network
-------------------------------
total reward:134
exploring: random action
total reward:143
exploiting: q-values predicted from network
-------------------------------
total reward:152
exploiting: q-values predicted from network
-------------------------------
total reward:161
exploiting: q-values predicted from network
-------------------------------
total reward:170
exploiting: q-values predicted from network
-------------------------------
total reward:179
exploiting: q-values predicted from network
-------------------------------
total reward:188
exploiting: q-values predicted from network
-------------------------------
total reward:197
exploring: random action
total reward:195
Episode: 93.01Total Reward: 195Total Steps: 24Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:18
exploring: random action
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploiting: q-values predicted from network
-------------------------------
total reward:90
exploiting: q-values predicted from network
-------------------------------
total reward:99
exploiting: q-values predicted from network
-------------------------------
total reward:108
exploiting: q-values predicted from network
-------------------------------
total reward:117
exploiting: q-values predicted from network
-------------------------------
total reward:126
exploiting: q-values predicted from network
-------------------------------
total reward:135
exploiting: q-values predicted from network
-------------------------------
total reward:144
exploiting: q-values predicted from network
-------------------------------
total reward:153
exploring: random action
total reward:152
exploiting: q-values predicted from network
-------------------------------
total reward:161
exploiting: q-values predicted from network
-------------------------------
total reward:170
exploiting: q-values predicted from network
-------------------------------
total reward:179
exploiting: q-values predicted from network
-------------------------------
total reward:188
exploiting: q-values predicted from network
-------------------------------
total reward:197
exploring: random action
total reward:195
Episode: 94.01Total Reward: 195Total Steps: 24Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploring: random action
total reward:35
exploiting: q-values predicted from network
-------------------------------
total reward:44
exploiting: q-values predicted from network
-------------------------------
total reward:53
exploiting: q-values predicted from network
-------------------------------
total reward:62
exploiting: q-values predicted from network
-------------------------------
total reward:71
exploring: random action
total reward:69
Episode: 95.01Total Reward: 69Total Steps: 10Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploring: random action
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploring: random action
total reward:79
Episode: 96.01Total Reward: 79Total Steps: 10Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploring: random action
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploiting: q-values predicted from network
-------------------------------
total reward:63
exploiting: q-values predicted from network
-------------------------------
total reward:72
exploiting: q-values predicted from network
-------------------------------
total reward:81
exploiting: q-values predicted from network
-------------------------------
total reward:90
exploring: random action
total reward:89
exploiting: q-values predicted from network
-------------------------------
total reward:98
exploiting: q-values predicted from network
-------------------------------
total reward:107
exploiting: q-values predicted from network
-------------------------------
total reward:116
exploiting: q-values predicted from network
-------------------------------
total reward:125
exploiting: q-values predicted from network
-------------------------------
total reward:134
exploiting: q-values predicted from network
-------------------------------
total reward:143
exploiting: q-values predicted from network
-------------------------------
total reward:152
exploiting: q-values predicted from network
-------------------------------
total reward:161
exploiting: q-values predicted from network
-------------------------------
total reward:170
exploiting: q-values predicted from network
-------------------------------
total reward:179
exploiting: q-values predicted from network
-------------------------------
total reward:188
exploiting: q-values predicted from network
-------------------------------
total reward:197
exploiting: q-values predicted from network
-------------------------------
total reward:206
exploiting: q-values predicted from network
-------------------------------
total reward:215
exploiting: q-values predicted from network
-------------------------------
total reward:224
exploiting: q-values predicted from network
-------------------------------
total reward:233
exploiting: q-values predicted from network
-------------------------------
total reward:242
exploiting: q-values predicted from network
-------------------------------
total reward:251
exploiting: q-values predicted from network
-------------------------------
total reward:260
exploring: random action
total reward:259
exploiting: q-values predicted from network
-------------------------------
total reward:268
exploiting: q-values predicted from network
-------------------------------
total reward:277
exploiting: q-values predicted from network
-------------------------------
total reward:286
exploring: random action
total reward:295
exploiting: q-values predicted from network
-------------------------------
total reward:304
exploiting: q-values predicted from network
-------------------------------
total reward:313
exploiting: q-values predicted from network
-------------------------------
total reward:322
exploiting: q-values predicted from network
-------------------------------
total reward:331
exploiting: q-values predicted from network
-------------------------------
total reward:340
exploiting: q-values predicted from network
-------------------------------
total reward:349
exploring: random action
total reward:347
Episode: 97.01Total Reward: 347Total Steps: 42Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploring: random action
total reward:34
Episode: 98.01Total Reward: 34Total Steps: 5Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
exploiting: q-values predicted from network
-------------------------------
total reward:9
exploiting: q-values predicted from network
-------------------------------
total reward:18
exploiting: q-values predicted from network
-------------------------------
total reward:27
exploiting: q-values predicted from network
-------------------------------
total reward:36
exploiting: q-values predicted from network
-------------------------------
total reward:45
exploiting: q-values predicted from network
-------------------------------
total reward:54
exploring: random action
total reward:52
Episode: 99.01Total Reward: 52Total Steps: 7Epsilon: 0.1853020188851842
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
actions:torch.Size([16, 6])
actions shape:torch.Size([16, 6, 1])
q-values:torch.Size([16, 6, 3])
q-expected:torch.Size([16, 6])
q-values next:torch.Size([16, 6, 3])
q-targets next:torch.Size([16, 6])
reward shape:torch.Size([16])
not done shape:torch.Size([16])
